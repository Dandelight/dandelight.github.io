# ConvNets!

神经网络是一个网络接一个网络，每个网络后面接一个非线性激活函数，比如$s = W_3 \max(0, W_2 \max(0, W_1 x))$就是非线性套非线性

## 从一个神经元开始

![img](media/04/neuron_model.jpeg)

单个的神经元可以认为是线性二分类器，对神经元施加的`weight decay`等正则化手段可以类比为“遗忘”。

## 非线性激活函数

TODO：如何画图？

![image-20211116155839498](media/04/image-20211116155839498.png)

最常用的、应该首先尝试的是`ReLU`。

## 神经网络架构

神经网络就是以神经元为节点的图。

但作为命名习惯，称一个$N$层神经网络时，输入层不计入。

## 前馈计算

就那么算。

## 数学原理

$$
\forall x, \mid f(x) - g(x) \mid < \epsilon
$$

其中，$f(x)$是目标函数，$g(x)$是神经网络拟合出的函数，故神经网络也被称为**通用函数拟合器universal function approximators**







## 数据预处理

### 减去均值

减掉均值，使数据的中点与原点重合

### 正则化

使向量各分量的数据尺度近似相同

### 权值初始化

误区：全是0，所有神经元将产生同样的梯度，所有的参数更新都是相同的，达不到训练效果

#### 小随机数

比较好一点，但是如果过小会出现梯度消失现象

####较推荐的初始化方式

使用`ReLU`激活单元，初始权值设置为`w = np.random.randn(n) * sqrt(2.0/n)`

### 批正则化

$$
\hat{x}^{(k)} = \frac{x^{(k)} - \operatorname{E}[x^{(k)}]}{\sqrt{\operatorname{Var}[x^{(k)}]}}
$$

### 正则化

#### L2正则化

That is, for every weight $w$ in the network, we add the term $\frac{1}{2} \lambda w^2$ to the objective, where $\lambda$ is the regularization strength.

#### L1正则化

$\lambda |w|$

#### Elastic net regularization

$\lambda_1 |w| + \lambda_2 w^2$

#### 最大范数约束

$||\vec w||_2 < c$，$c$的数量级在$3$~$4$。

#### Dropout

随机将一些神经元的输出置$0$。

但这也带来了新的问题：假设dropout概率为$0.5$，测试时不使用dropout，那么要将dropout层的输出除以$2$，下一层的神经元才能接收到合理的输出。这就造成了在测试时的多余计算。所以，使用inverse dropout解决，即在训练时就将dropout层的结构乘以$1/p$，测试时原样输出。



### In Practice

常用做法是使用一个全局L2正则化，交叉验证，每一层后都有dropout，默认设置$p=0.5$。



## 损失函数

### 分类

分类的损失函数有SVM loss
$$
L_i = \sum_{j\neq y_i} \max(0, f_j - f_{y_i} + 1)
$$
或者有人用平方脊损失$\max(0, f_j - f_{y_i} + 1)^2$

cross-entropy loss
$$
L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)
$$
或者对每一个类训练一个logistic regression
$$
P(y = 1 \mid x; w, b) = \frac{1}{1 + e^{-(w^Tx +b)}} = \sigma (w^Tx + b)
$$
加起来就成了这样：
$$
L_i = -\sum_j y_{ij} \log(\sigma(f_j)) + (1 - y_{ij}) \log(1 - \sigma(f_j))
$$
上边式子看起来复杂，其实$f$的导数非常简单明了：$\frac{\partial L_i}{\partial f_j} = \sigma(f_j)-y_{ij}$

### 回归

最简单的就是L1范式$L_i = ||f-y_i||_1$和L2范式的平方$L_i = ||f-y_i||^2_2$。

